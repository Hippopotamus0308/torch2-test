{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUR62XGb5GfP/0C1Ln2L4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hippopotamus0308/torch2-test/blob/feat-basic-test/torch2_test_compile_mode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzSxLQhqorzU",
        "outputId": "2e07dc68-986d-461e-92bb-4903551af022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cpu, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-2.0.0.dev20230116%2Bcpu-cp38-cp38-linux_x86_64.whl (194.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading https://download.pytorch.org/whl/nightly/networkx-3.0rc1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading https://download.pytorch.org/whl/nightly/sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading https://download.pytorch.org/whl/nightly/typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading https://download.pytorch.org/whl/nightly/mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, networkx, torch\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.2.1\n",
            "    Uninstalling mpmath-1.2.1:\n",
            "      Successfully uninstalled mpmath-1.2.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.7.1\n",
            "    Uninstalling sympy-1.7.1:\n",
            "      Successfully uninstalled sympy-1.7.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 2.8.8\n",
            "    Uninstalling networkx-2.8.8:\n",
            "      Successfully uninstalled networkx-2.8.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 2.0.0.dev20230116+cpu which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 2.0.0.dev20230116+cpu which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 2.0.0.dev20230116+cpu which is incompatible.\n",
            "fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 2.0.0.dev20230116+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mpmath-1.2.1 networkx-3.0rc1 sympy-1.11.1 torch-2.0.0.dev20230116+cpu typing-extensions-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch._dynamo\n",
        "from typing import List\n",
        "import time\n",
        "\n",
        "def timed(fn):\n",
        "    start = time.time()\n",
        "    result = fn()\n",
        "    end = time.time()\n",
        "    time_cnt = end - start\n",
        "    #print(f\"{printer}, time: {time_cnt}\")\n",
        "    return result, time_cnt\n",
        "\n",
        "\n",
        "def generate_data(b):\n",
        "    return (\n",
        "        torch.randn(b, 3, 128, 128).to(torch.float32),\n",
        "        torch.randint(1000, (b,)),\n",
        "    )"
      ],
      "metadata": {
        "id": "tPO4JPS6p-ha"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_-ls21AsGko",
        "outputId": "4f36d2dc-3646-40f8-c28d-2bdb3e5ebf5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt_model_default = torch.compile(model, mode=\"default\")\n",
        "opt_model_reduce_overhead = torch.compile(model, mode=\"reduce-overhead\")\n",
        "opt_model_max_autotune = torch.compile(model, mode=\"max-autotune\")"
      ],
      "metadata": {
        "id": "lJVeDKK0sPwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0759c223-cebc-48d4-faa4-d9be4cf4b7a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  opt_model_default(generate_data(2)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIkwwKzKeOeB",
        "outputId": "da5c74cc-912b-4def-82a6-d99e48a973fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torch._functorch.aot_autograd:Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1368, in aot_wrapper_dedupe\n",
            "    fw_metadata, _out = run_functionalized_fw_and_collect_metadata(flat_fn)(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 569, in inner\n",
            "    flat_f_outs = f(*flat_f_args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 2415, in functional_call\n",
            "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
            "    self.env[node] = self.run_node(node)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
            "    return getattr(self, n.op)(n.target, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
            "    return submod(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1488, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 337, in forward\n",
            "    return self._conv_forward(input, other, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 320, in _conv_forward\n",
            "    return torch.ops.mkldnn._convolution_pointwise_(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/overrides.py\", line 37, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "RuntimeError: !schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED at \"../aten/src/ATen/FunctionalizeFallbackKernel.cpp\":32, please report a bug to PyTorch. mutating and aliasing ops should all have codegen'd kernels\n",
            "\n",
            "While executing %self_layer2_0_downsample_0 : [#users=2] = call_module[target=self_layer2_0_downsample_0](args = (%self_layer1_1_conv2, %self_layer2_0_conv2), kwargs = {})\n",
            "Original traceback:\n",
            "  File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 78, in forward\n",
            "    identity = self.downsample(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 238, in _forward_impl\n",
            "    x = self.layer2(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 249, in forward\n",
            "    return self._forward_impl(x)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(cnt):\n",
        "  time_no_opt = []\n",
        "  time_default = []\n",
        "  time_reduce_overhead = []\n",
        "  time_max_autotune = []\n",
        "\n",
        "  ## warm up\n",
        "  for i in range(5):\n",
        "    with torch.no_grad():\n",
        "      model(generate_data(cnt)[0])\n",
        "      opt_model_default(generate_data(cnt)[0])\n",
        "      opt_model_reduce_overhead(generate_data(cnt)[0])\n",
        "      opt_model_max_autotune(generate_data(cnt)[0])\n",
        "\n",
        "  for i in range(10):\n",
        "    with torch.no_grad():\n",
        "      _, time1 = timed(lambda:model(generate_data(cnt)[0]))\n",
        "      _, time2 = timed(lambda:opt_model_default(generate_data(cnt)[0]))\n",
        "      _, time3 = timed(lambda:opt_model_reduce_overhead(generate_data(cnt)[0]))\n",
        "      _, time4 = timed(lambda:opt_model_max_autotune(generate_data(cnt)[0]))\n",
        "    time_no_opt.append(time1)\n",
        "    time_default.append(time2)\n",
        "    time_reduce_overhead.append(time3)\n",
        "    time_max_autotune.append(time4)   \n",
        "\n",
        "  no_opt_median_time = np.median(time_no_opt)\n",
        "  default_opt_median_time = np.median(time_default)\n",
        "  ro_median_time = np.median(time_reduce_overhead)\n",
        "  ma_median_time = np.median(time_max_autotune)\n",
        "\n",
        "  no_opt_mean_time = np.mean(time_no_opt)\n",
        "  default_opt_mean_time = np.mean(time_default)\n",
        "  ro_mean_time = np.mean(time_reduce_overhead)\n",
        "  ma_mean_time = np.mean(time_max_autotune)\n",
        "\n",
        "  print(\"-------------Median Time---------------\")\n",
        "  print(f\"no opt median time: {no_opt_median_time}\")\n",
        "  print(f\"mode = defualt: {default_opt_median_time}\")\n",
        "  print(f\"mode = reduce overhead: {ro_median_time}\")\n",
        "  print(f\"mode = max autotune: {ma_median_time}\")\n",
        "\n",
        "  print(\"-------------Mean Time---------------\")\n",
        "  print(f\"no opt mean time: {no_opt_mean_time}\")\n",
        "  print(f\"mode = defualt: {default_opt_mean_time}\")\n",
        "  print(f\"mode = reduce overhead: {ro_mean_time}\")\n",
        "  print(f\"mode = max autotune: {ma_mean_time}\")  "
      ],
      "metadata": {
        "id": "zJcdCA21taWo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y722bOAxtEy6",
        "outputId": "26ce5fe6-4054-4fdf-ba90-32618daa3d74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torch._functorch.aot_autograd:Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1368, in aot_wrapper_dedupe\n",
            "    fw_metadata, _out = run_functionalized_fw_and_collect_metadata(flat_fn)(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 569, in inner\n",
            "    flat_f_outs = f(*flat_f_args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 2415, in functional_call\n",
            "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
            "    self.env[node] = self.run_node(node)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
            "    return getattr(self, n.op)(n.target, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
            "    return submod(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1488, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 337, in forward\n",
            "    return self._conv_forward(input, other, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 320, in _conv_forward\n",
            "    return torch.ops.mkldnn._convolution_pointwise_(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/overrides.py\", line 37, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "RuntimeError: !schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED at \"../aten/src/ATen/FunctionalizeFallbackKernel.cpp\":32, please report a bug to PyTorch. mutating and aliasing ops should all have codegen'd kernels\n",
            "\n",
            "While executing %self_layer2_0_downsample_0 : [#users=2] = call_module[target=self_layer2_0_downsample_0](args = (%self_layer1_1_conv2, %self_layer2_0_conv2), kwargs = {})\n",
            "Original traceback:\n",
            "  File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 78, in forward\n",
            "    identity = self.downsample(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 238, in _forward_impl\n",
            "    x = self.layer2(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 249, in forward\n",
            "    return self._forward_impl(x)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 0.03492546081542969\n",
            "mode = defualt: 0.027805447578430176\n",
            "mode = reduce overhead: 0.028104186058044434\n",
            "mode = max autotune: 0.028142809867858887\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 0.03588788509368897\n",
            "mode = defualt: 0.02912609577178955\n",
            "mode = reduce overhead: 0.02819325923919678\n",
            "mode = max autotune: 0.02925412654876709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCNMpHVEwNUE",
        "outputId": "b9ce046f-ecbd-4f08-f8c2-c33084145dbe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torch._functorch.aot_autograd:Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1368, in aot_wrapper_dedupe\n",
            "    fw_metadata, _out = run_functionalized_fw_and_collect_metadata(flat_fn)(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 569, in inner\n",
            "    flat_f_outs = f(*flat_f_args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 2415, in functional_call\n",
            "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
            "    self.env[node] = self.run_node(node)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
            "    return getattr(self, n.op)(n.target, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
            "    return submod(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1488, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 337, in forward\n",
            "    return self._conv_forward(input, other, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 320, in _conv_forward\n",
            "    return torch.ops.mkldnn._convolution_pointwise_(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/overrides.py\", line 37, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "RuntimeError: !schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED at \"../aten/src/ATen/FunctionalizeFallbackKernel.cpp\":32, please report a bug to PyTorch. mutating and aliasing ops should all have codegen'd kernels\n",
            "\n",
            "While executing %self_layer2_0_downsample_0 : [#users=2] = call_module[target=self_layer2_0_downsample_0](args = (%self_layer1_1_conv2, %self_layer2_0_conv2), kwargs = {})\n",
            "Original traceback:\n",
            "  File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 78, in forward\n",
            "    identity = self.downsample(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 238, in _forward_impl\n",
            "    x = self.layer2(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 249, in forward\n",
            "    return self._forward_impl(x)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 0.23064863681793213\n",
            "mode = defualt: 0.1922684907913208\n",
            "mode = reduce overhead: 0.19235754013061523\n",
            "mode = max autotune: 0.1908893585205078\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 0.23183903694152833\n",
            "mode = defualt: 0.19224832057952881\n",
            "mode = reduce overhead: 0.19236133098602295\n",
            "mode = max autotune: 0.1904613494873047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5abgXwj6wQdA",
        "outputId": "0f7636fa-1cf6-430d-d7b3-78a1ddc5f092"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 0.8679666519165039\n",
            "mode = defualt: 0.7437078952789307\n",
            "mode = reduce overhead: 0.7450851202011108\n",
            "mode = max autotune: 0.7439479827880859\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 0.9924789428710937\n",
            "mode = defualt: 0.8488427639007569\n",
            "mode = reduce overhead: 0.8483895063400269\n",
            "mode = max autotune: 0.9141491174697876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Bn5hyzwRrC",
        "outputId": "e1463213-505b-4666-eb01-bbbb60684278"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torch._functorch.aot_autograd:Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1368, in aot_wrapper_dedupe\n",
            "    fw_metadata, _out = run_functionalized_fw_and_collect_metadata(flat_fn)(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 569, in inner\n",
            "    flat_f_outs = f(*flat_f_args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 2415, in functional_call\n",
            "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
            "    self.env[node] = self.run_node(node)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
            "    return getattr(self, n.op)(n.target, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
            "    return submod(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1488, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 337, in forward\n",
            "    return self._conv_forward(input, other, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 320, in _conv_forward\n",
            "    return torch.ops.mkldnn._convolution_pointwise_(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/overrides.py\", line 37, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "RuntimeError: !schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED at \"../aten/src/ATen/FunctionalizeFallbackKernel.cpp\":32, please report a bug to PyTorch. mutating and aliasing ops should all have codegen'd kernels\n",
            "\n",
            "While executing %self_layer2_0_downsample_0 : [#users=2] = call_module[target=self_layer2_0_downsample_0](args = (%self_layer1_1_conv2, %self_layer2_0_conv2), kwargs = {})\n",
            "Original traceback:\n",
            "  File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 78, in forward\n",
            "    identity = self.downsample(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 238, in _forward_impl\n",
            "    x = self.layer2(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 249, in forward\n",
            "    return self._forward_impl(x)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 1.7157058715820312\n",
            "mode = defualt: 1.477898120880127\n",
            "mode = reduce overhead: 1.4723057746887207\n",
            "mode = max autotune: 1.4827030897140503\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 1.7151725053787232\n",
            "mode = defualt: 1.4786361694335937\n",
            "mode = reduce overhead: 1.4722907781600951\n",
            "mode = max autotune: 1.4845173358917236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaqqt6zTwTBk",
        "outputId": "f469ada6-624c-4f50-b615-07a0a37fd419"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torch._functorch.aot_autograd:Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1368, in aot_wrapper_dedupe\n",
            "    fw_metadata, _out = run_functionalized_fw_and_collect_metadata(flat_fn)(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 569, in inner\n",
            "    flat_f_outs = f(*flat_f_args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 2415, in functional_call\n",
            "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
            "    self.env[node] = self.run_node(node)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
            "    return getattr(self, n.op)(n.target, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
            "    return submod(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1488, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 337, in forward\n",
            "    return self._conv_forward(input, other, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/mkldnn.py\", line 320, in _conv_forward\n",
            "    return torch.ops.mkldnn._convolution_pointwise_(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/overrides.py\", line 37, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 499, in __call__\n",
            "    return self._op(*args, **kwargs or {})\n",
            "RuntimeError: !schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED at \"../aten/src/ATen/FunctionalizeFallbackKernel.cpp\":32, please report a bug to PyTorch. mutating and aliasing ops should all have codegen'd kernels\n",
            "\n",
            "While executing %self_layer2_0_downsample_0 : [#users=2] = call_module[target=self_layer2_0_downsample_0](args = (%self_layer1_1_conv2, %self_layer2_0_conv2), kwargs = {})\n",
            "Original traceback:\n",
            "  File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 78, in forward\n",
            "    identity = self.downsample(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 238, in _forward_impl\n",
            "    x = self.layer2(x)\n",
            " |   File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\", line 249, in forward\n",
            "    return self._forward_impl(x)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 3.4792494773864746\n",
            "mode = defualt: 3.0186045169830322\n",
            "mode = reduce overhead: 3.0502309799194336\n",
            "mode = max autotune: 3.0051430463790894\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 3.516086387634277\n",
            "mode = defualt: 3.1154479503631594\n",
            "mode = reduce overhead: 3.2574779987335205\n",
            "mode = max autotune: 3.2143298387527466\n"
          ]
        }
      ]
    }
  ]
}