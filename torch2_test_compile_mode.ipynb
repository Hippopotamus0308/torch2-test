{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+wu0mWb9G/IZuA8AdNnjK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hippopotamus0308/torch2-test/blob/feat-basic-test/torch2_test_compile_mode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzSxLQhqorzU",
        "outputId": "371f0bcc-f042-4064-bb69-6af286844937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cpu, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cpu/torch-2.0.0.dev20230105%2Bcpu-cp38-cp38-linux_x86_64.whl (194.4 MB)\n",
            "Collecting networkx\n",
            "  Using cached https://download.pytorch.org/whl/nightly/networkx-3.0rc1-py3-none-any.whl (2.0 MB)\n",
            "Collecting typing-extensions\n",
            "  Using cached https://download.pytorch.org/whl/nightly/typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting sympy\n",
            "  Using cached https://download.pytorch.org/whl/nightly/sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "Collecting mpmath>=0.19\n",
            "  Using cached https://download.pytorch.org/whl/nightly/mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
            "Installing collected packages: mpmath, typing-extensions, sympy, networkx, torch\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.2.1\n",
            "    Uninstalling mpmath-1.2.1:\n",
            "      Successfully uninstalled mpmath-1.2.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.11.1\n",
            "    Uninstalling sympy-1.11.1:\n",
            "      Successfully uninstalled sympy-1.11.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0rc1\n",
            "    Uninstalling networkx-3.0rc1:\n",
            "      Successfully uninstalled networkx-3.0rc1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0.dev20230105+cpu\n",
            "    Uninstalling torch-2.0.0.dev20230105+cpu:\n",
            "      Successfully uninstalled torch-2.0.0.dev20230105+cpu\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 2.0.0.dev20230105+cpu which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 2.0.0.dev20230105+cpu which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 2.0.0.dev20230105+cpu which is incompatible.\n",
            "fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 2.0.0.dev20230105+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mpmath-1.2.1 networkx-3.0rc1 sympy-1.11.1 torch-2.0.0.dev20230105+cpu typing-extensions-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch._dynamo\n",
        "from typing import List\n",
        "import time\n",
        "\n",
        "def timed(fn):\n",
        "    start = time.time()\n",
        "    result = fn()\n",
        "    end = time.time()\n",
        "    time_cnt = end - start\n",
        "    #print(f\"{printer}, time: {time_cnt}\")\n",
        "    return result, time_cnt\n",
        "\n",
        "\n",
        "def generate_data(b):\n",
        "    return (\n",
        "        torch.randn(b, 3, 128, 128).to(torch.float32),\n",
        "        torch.randint(1000, (b,)),\n",
        "    )"
      ],
      "metadata": {
        "id": "tPO4JPS6p-ha"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_-ls21AsGko",
        "outputId": "42ddcbd3-b38d-4cfc-88dc-b1272052f3fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 87.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt_model_default = torch.compile(model, mode=\"default\")\n",
        "opt_model_reduce_overhead = torch.compile(model, mode=\"reduce-overhead\")\n",
        "opt_model_max_autotune = torch.compile(model, mode=\"max-autotune\")"
      ],
      "metadata": {
        "id": "lJVeDKK0sPwK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nelC7AE-svDL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(cnt):\n",
        "  time_no_opt = []\n",
        "  time_default = []\n",
        "  time_reduce_overhead = []\n",
        "  time_max_autotune = []\n",
        "\n",
        "  ## warm up\n",
        "  for i in range(5):\n",
        "    model(generate_data(cnt)[0])\n",
        "    opt_model_default(generate_data(cnt)[0])\n",
        "    opt_model_reduce_overhead(generate_data(cnt)[0])\n",
        "    opt_model_max_autotune(generate_data(cnt)[0])\n",
        "\n",
        "  for i in range(10):\n",
        "    _, time1 = timed(lambda:model(generate_data(cnt)[0]))\n",
        "    _, time2 = timed(lambda:opt_model_default(generate_data(cnt)[0]))\n",
        "    _, time3 = timed(lambda:opt_model_reduce_overhead(generate_data(cnt)[0]))\n",
        "    _, time4 = timed(lambda:opt_model_max_autotune(generate_data(cnt)[0]))\n",
        "    time_no_opt.append(time1)\n",
        "    time_default.append(time2)\n",
        "    time_reduce_overhead.append(time3)\n",
        "    time_max_autotune.append(time4)   \n",
        "\n",
        "  no_opt_median_time = np.median(time_no_opt)\n",
        "  default_opt_median_time = np.median(time_default)\n",
        "  ro_median_time = np.median(time_reduce_overhead)\n",
        "  ma_median_time = np.median(time_max_autotune)\n",
        "\n",
        "  no_opt_mean_time = np.mean(time_no_opt)\n",
        "  default_opt_mean_time = np.mean(time_default)\n",
        "  ro_mean_time = np.mean(time_reduce_overhead)\n",
        "  ma_mean_time = np.mean(time_max_autotune)\n",
        "\n",
        "  print(\"-------------Median Time---------------\")\n",
        "  print(f\"no opt median time: {no_opt_median_time}\")\n",
        "  print(f\"mode = defualt: {default_opt_median_time}\")\n",
        "  print(f\"mode = reduce overhead: {ro_median_time}\")\n",
        "  print(f\"mode = max autotune: {ma_median_time}\")\n",
        "\n",
        "  print(\"-------------Mean Time---------------\")\n",
        "  print(f\"no opt mean time: {no_opt_mean_time}\")\n",
        "  print(f\"mode = defualt: {default_opt_mean_time}\")\n",
        "  print(f\"mode = reduce overhead: {ro_mean_time}\")\n",
        "  print(f\"mode = max autotune: {ma_mean_time}\")  "
      ],
      "metadata": {
        "id": "zJcdCA21taWo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y722bOAxtEy6",
        "outputId": "fe081a73-838c-4e9a-8870-47dca241abf3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 0.03913414478302002\n",
            "mode = defualt: 0.04544544219970703\n",
            "mode = reduce overhead: 0.04502689838409424\n",
            "mode = max autotune: 0.044536471366882324\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 0.0397219181060791\n",
            "mode = defualt: 0.04786083698272705\n",
            "mode = reduce overhead: 0.04614570140838623\n",
            "mode = max autotune: 0.044898605346679686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCNMpHVEwNUE",
        "outputId": "9054fe9d-0ce0-40f7-e92b-e268f31b3638"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 0.2674351930618286\n",
            "mode = defualt: 0.3092167377471924\n",
            "mode = reduce overhead: 0.3070477247238159\n",
            "mode = max autotune: 0.3095734119415283\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 0.26920514106750487\n",
            "mode = defualt: 0.3109787702560425\n",
            "mode = reduce overhead: 0.3109633684158325\n",
            "mode = max autotune: 0.31057398319244384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5abgXwj6wQdA",
        "outputId": "58b3bfc2-5663-43c9-9a7e-abbdffaf635b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 0.9859806299209595\n",
            "mode = defualt: 1.176600456237793\n",
            "mode = reduce overhead: 1.1663029193878174\n",
            "mode = max autotune: 1.1761353015899658\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 0.9861690998077393\n",
            "mode = defualt: 1.1880727529525756\n",
            "mode = reduce overhead: 1.1696454524993896\n",
            "mode = max autotune: 1.2120518207550048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Bn5hyzwRrC",
        "outputId": "d08a479c-b895-4783-c70b-d03e25c08d82"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 1.9487462043762207\n",
            "mode = defualt: 2.295313000679016\n",
            "mode = reduce overhead: 2.332722306251526\n",
            "mode = max autotune: 2.3192657232284546\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 1.9546403408050537\n",
            "mode = defualt: 2.319568729400635\n",
            "mode = reduce overhead: 2.3309523105621337\n",
            "mode = max autotune: 2.3204650402069094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaqqt6zTwTBk",
        "outputId": "070b4c72-888d-4ecf-e4d8-da8e780a3c3f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Median Time---------------\n",
            "no opt median time: 3.7803475856781006\n",
            "mode = defualt: 4.5058698654174805\n",
            "mode = reduce overhead: 4.5549890995025635\n",
            "mode = max autotune: 4.49209189414978\n",
            "-------------Mean Time---------------\n",
            "no opt mean time: 3.7929439306259156\n",
            "mode = defualt: 4.61926236152649\n",
            "mode = reduce overhead: 4.547304439544678\n",
            "mode = max autotune: 4.498769235610962\n"
          ]
        }
      ]
    }
  ]
}